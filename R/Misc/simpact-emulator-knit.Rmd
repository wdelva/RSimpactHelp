---
title: "Simpact Emulator"
author: "Trust Chibawara"
date: "03 November 2016"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r getready, echo = FALSE, message = FALSE, cache = FALSE}
pacman::p_load(RSimpactCyan, RSimpactHelper, data.table, dplyr, magrittr, exactci,
               nlme, ggplot2,survival, KMsurv, tidyr, expoTree, sna, intergraph,
               igraph,lhs, GGally, emulator, multivator, tidyr, psych)
# install.packages("readcsvcolumns", repos="http://193.190.10.42/jori/")
```
## Export Simpact Simulation Runs

The simpact emulator is a calibration procedure aimed at obtaining sutaible parameters that will best fit the target statistics. To make this possible we make use of the simulated data from simpact.run with where selected parameters are used and as output are summary statistics.

We then teach the emulator that given these parameters, these are the summary statistics that are produced. We then give it our target statistics and ask what are the optimal parameters that should produce these statistics.


```{r readchunckoutputs, echo=FALSE}
dirname <- "/home/trust/Documents/GIT_Projects/RSimpactHelp/" #### Change this as needed
#We can also use the director list to read directly .csv and chunk in file name.
# output file from the simpact.run with selected design.points
all.chunks.processed <- c("SummaryOutPut-inANDout.df.chunk-1-1000-2016-11-05-2.csv",
                         "SummaryOutPut-inANDout.df.chunk-1-1000-2016-11-05-3.csv")

file.name.csv <- paste0(dirname, "SummaryOutPut-inANDout.df.chunk-1-1000-2016-11-05-1.csv") # param.varied
# Read the output file from running simpact many times.
inputANDoutput.complete <- read.csv(file = file.name.csv, header = TRUE)
for (chunk.df in all.chunks.processed){
  chuck.df.filename <- paste0(dirname, chunk.df) # param.varied
  chunk.df.data <- read.csv(chuck.df.filename)
  chunk.df.data <- data.frame(chunk.df.data)

  inputANDoutput.complete <- rbind(inputANDoutput.complete, chunk.df.data)
}

summary.count <- which(colnames(inputANDoutput.complete)=="sim.id") - 1
xdesign.count <- length(names(dplyr::select(inputANDoutput.complete, contains("xdesign"))))
simpact.par.count <- length(inputANDoutput.complete) - summary.count - xdesign.count - 1

repetition.count <- nrow(inputANDoutput.complete[inputANDoutput.complete$sim.id==1,])

```
## Data Set To Be Used For Calibration

There are `r simpact.par.count` Simpact parameters to be calibrated, which are:
```{r simpactparameters, echo=FALSE}
names(inputANDoutput.complete[,(summary.count+xdesign.count+2):length(inputANDoutput.complete)])
```

And `r summary.count` emulator training summary statistics given as: 
```{r summaryparameters, echo=FALSE}
summaryparameters <- names(inputANDoutput.complete[,1:summary.count])
summaryparameters
```
Because some of the parameters to be calibrated with be equal, the `r simpact.par.count` unique list are the ones which where used to generate the above parameter sets.

The data simulated is a repetition = `r repetition.count` of each parameter combination. We will only consider here those combinations that all produced valid summary statistics (no NA's) and those whose prev.men.15.25 > 0.05.

Moreover, we might be interested in using some of the summary statistics not necessariry all that were generated from the simulation. We provide these below as indicated. And for the emulation exercise to be reliable the training target statistics needs to be normally distributed.

The figure below shows the distribution of the summary data to be used. We might need to transform any of the summary statistics should the histogram diverge from normaility.

```{r simpactparametsinput, echo=FALSE}
#summary statistics
z.variables <- c("growth.rate", "median.AD", "Q1.AD", "Q3.AD", "prev.men.15.25", "prev.men.25.50",
                 "ART.cov.15.50")

# The x variables (model parameters) that were varied:
x.variables <- c("person.eagerness.man.dist.gamma.a", "person.eagerness.man.dist.gamma.b", "conception.alpha_base",
                 "formation.hazard.agegapry.numrel_man", "formation.hazard.agegapry.eagerness_diff",
                 "formation.hazard.agegapry.gap_factor_man_exp", "person.agegap.man.dist.normal.mu",
                 "person.agegap.woman.dist.normal.mu","person.agegap.man.dist.normal.sigma",
                 "person.agegap.woman.dist.normal.sigma")

x.variables.boundaries <- list(person.eagerness.man.dist.gamma.a.min =0.1, person.eagerness.man.dist.gamma.a.max =2,
                    person.eagerness.man.dist.gamma.b.min = 5, person.eagerness.man.dist.gamma.b.max = 60,
                    conception.alpha_base.min = -3.6, conception.alpha_base.max = -1.2,
                    formation.hazard.agegapry.numrel_man.min = -1.5, formation.hazard.agegapry.numrel_man.max = -0.1,
                    formation.hazard.agegapry.eagerness_diff.min = -0.1, formation.hazard.agegapry.eagerness_diff.max = 0,
                    formation.hazard.agegapry.gap_factor_woman_exp.min = -1.5, formation.hazard.agegapry.gap_factor_woman_exp.max =-0.4,
                    person.agegap.man.dist.normal.mu.min = 0, person.agegap.man.dist.normal.mu.max = 4,
                    person.agegap.woman.dist.normal.mu.min =0, person.agegap.woman.dist.normal.mu.max = 4,
                    person.agegap.man.dist.normal.sigma.min = 0.5, person.agegap.man.dist.normal.sigma.max =2,
                    person.agegap.woman.dist.normal.sigma.min =0.5, person.agegap.woman.dist.normal.sigma.max =2)

#select the

## Decide if we want to keep only rows without NA or
inputANDoutput.select <- dplyr::filter(inputANDoutput.complete,
                                     complete.cases(inputANDoutput.complete[,z.variables]),
                                     prev.men.15.25 > 0.05)
# remove all rows which do not have sim.id occurring the number of repeat times.
freq.sim.id <- plyr::count(inputANDoutput.select,"sim.id")
#get all the average in all the columns in the selected df
#inputANDoutput.select <- inputANDoutput.select %>% dplyr::group_by(sim.id) %>% dplyr::summarise_each(funs(mean))

inputANDoutput.select <- aggregate(inputANDoutput.select, by = list(inputANDoutput.select$sim.id), FUN = "mean")

inputANDoutput.select <- left_join(freq.sim.id, inputANDoutput.select, by = "sim.id")
inputANDoutput.select <- inputANDoutput.select[inputANDoutput.select$freq == repetition.count,]

```


```{r targetsandfilter, echo=FALSE}
#Set the targets for the summary statistics.
targets <- c(0.014, 3, 2, 5, 0.08, 0.25, 0.3)

#Select the first 250 testing the optiomal_paras()
#You can also select a fraction of simulated dataset set round(dim(simpact.inANDout.df)[1]*0.10, digits=0)
inputANDoutput.select <- head(inputANDoutput.select, 250)

#select the x model param values (model parameters)
simpact.x <- dplyr::select_(inputANDoutput.select,.dots=x.variables) %>% as.matrix()
#select the z model param values (summary statistics)
simpact.z <- dplyr::select_(inputANDoutput.select,.dots=z.variables)
#select the x.design frame
x.design.name <- names(dplyr::select(inputANDoutput.select, contains("xdesign")))
x.design <- dplyr::select_(inputANDoutput.select,.dots=x.design.name)

### Check is z.variables and x.variables are in inANDout.df ####

try(if(length(targets)!=length(z.variables)) stop("Target values are not equal to the variables set"))

```

```{r histogram, echo=FALSE}
par(mfrow=c(1,1))
multi.hist(simpact.z)
```
## Baseline Statistics

If we take the simulated data and ask what are the best baseline parameters that give statistics that are close to our target statistics. (We should be able to compare these with the procedural parameters that we would obtain). 

The summary statistics are: 
```{r explorecurrentfit, echo=FALSE}
##Creating a LHS for each summary statistic
x.design.long <- x.design[rep(1:nrow(x.design),length(z.variables)),]
x.design.long <- as.matrix(x.design.long)
#### Computing model output at design points
z_obs <- as.vector(unlist(simpact.z))

#### Before we start the emulation, let's see what the best fit is from the model simulation runs
z.df <- simpact.z
sq.array <- as.data.frame(t(apply(z.df, 1, function(x) (x - t(targets))^2)))
SumSq <- as.numeric(rowSums(sq.array))
sim.best.summary <- z.df[which.min(SumSq), ]
#### And most importantly, the best estimate for the model parameters:
x.estimate <- as.numeric(simpact.x[which.min(SumSq), ])

p.stats <- rbind(sim.best.summary, targets)
rownames(p.stats) <- c("Summary Statistics","Targets")

p.stats
```

And the simpact parameters are:

`r x.estimate`

## Principle Componet Analysis

This will allow us to decide if we will be using all the target statistics or some of the variance can be explained by a few of these statistics. From the histogram below, we see that  the first four components explains most of the variance. Hence we will now use these four to build our emulator.

```{r principlecomponetanalysis, echo=FALSE}
z.pc <- princomp(z.df, scores = TRUE, cor = TRUE)
cum.prop.var <- summary(z.pc) # The first 4 components capture 94% of the total variance. That's perhaps sufficient?
comp.number.to.use <- 4 #change this if 5 which captures 98% is better.
#biplot(z.pc)
#z.pc$loadings
z.pc.df <- data.frame(z.pc$scores)
z.pc.obs <- as.vector(unlist(z.pc.df[ ,1:comp.number.to.use]))

## Decide if you will drop some of the summary statics
x.design.pc.long <- x.design[rep(1:nrow(x.design),comp.number.to.use),]
x.design.pc.long <- as.matrix(x.design.pc.long)

```
```{r histogrampca, echo=FALSE}
plot(z.pc, main = "PCA Variance Contribution per Component")
```

## PCA Multivator Objects 

Note: Should the number of components changes, we need to add a few lines here to indicate this. At the moment we are working with `r comp.number.to.use`. If this number is less or greater than 4, then we need to add/remove checks below.

Creating the multivator objects requires the use of optimal_params function which is very compute intensive. Hence we will use the option "a" initially and then we use the values from that option as a feed to option "b", then iterate through in small chunk iteration values and keep track of convergence of the estimated coeficient values that will be use for the experimentation by the emulator.
```{r mulitvatorobjects, echo=FALSE}
#### Creating the multivator objects for the PCA-based analysis
RS.pc.mdm <- mdm(x.design.pc.long, types = rep(names(z.pc.df)[1:comp.number.to.use], each = dim(simpact.z)[1]))
RS.pc.expt <- experiment(mm = RS.pc.mdm, obs = z.pc.obs)
#
optima.starttime.pc <- proc.time()
RS.pc.opt.a <- optimal_params(RS.pc.expt, option="a")
optima.endtime.pc <- proc.time() - optima.starttime.pc
#
comp1.pc.B <- data.frame(t(diag(B(RS.pc.opt.a)[,,1])),"a")
comp2.pc.B <- data.frame(t(diag(B(RS.pc.opt.a)[,,2])),"a")
comp3.pc.B <- data.frame(t(diag(B(RS.pc.opt.a)[,,3])),"a")
comp4.pc.B <- data.frame(t(diag(B(RS.pc.opt.a)[,,4])),"a")

names(comp1.pc.B)[names(comp1.pc.B)=="X.a."] <- "run.type"
names(comp2.pc.B)[names(comp2.pc.B)=="X.a."] <- "run.type"
names(comp3.pc.B)[names(comp3.pc.B)=="X.a."] <- "run.type"
names(comp4.pc.B)[names(comp4.pc.B)=="X.a."] <- "run.type"

RS.opt.pc.var <-  RS.pc.opt.a

optim.check.pc <- proc.time()

```

```{r convergencycheck, echo=FALSE}
## Use the loop to get iterate through different values. So the optimasation process is faster.

for (iter in seq(100,700, 100)){
 print (paste("Working on iteration number: ", iter, sep=" "))
 RS.opt.b.var.iter <- optimal_params(RS.pc.expt, option="b", start_hp = RS.opt.pc.var, control = list(maxit=iter))
 RS.opt.pc.var <- RS.opt.b.var.iter
#
 comp1.pc.B.var <- data.frame(t(diag(B(RS.opt.b.var.iter)[,,1])),paste("b",iter,sep = ""))
 comp2.pc.B.var <- data.frame(t(diag(B(RS.opt.b.var.iter)[,,2])),paste("b",iter,sep = ""))
 comp3.pc.B.var <- data.frame(t(diag(B(RS.opt.b.var.iter)[,,3])),paste("b",iter,sep = ""))
 comp4.pc.B.var <- data.frame(t(diag(B(RS.opt.b.var.iter)[,,4])),paste("b",iter,sep = ""))
#
 names(comp1.pc.B.var)[11] <- "run.type"
 names(comp2.pc.B.var)[11] <- "run.type"
 names(comp3.pc.B.var)[11] <- "run.type"
 names(comp4.pc.B.var)[11] <- "run.type"
#
 comp1.pc.B <- rbind(comp1.pc.B, comp1.pc.B.var)
 comp2.pc.B <- rbind(comp2.pc.B, comp2.pc.B.var)
 comp3.pc.B <- rbind(comp3.pc.B, comp3.pc.B.var)
 comp4.pc.B <- rbind(comp4.pc.B, comp4.pc.B.var)
}
#check how long this took.
RS.pc.opt.b <- RS.opt.pc.var
optim.check.pc.conv <- proc.time() - optim.check.pc
#
#See the plot of convergency in the B matrix of coefficients.
ggplot(melt(comp1.pc.B,id.vars=c("run.type")), aes(x=run.type,y=value, color=variable)) + geom_point() + facet_grid(variable ~ .)
ggplot(melt(comp2.pc.B,id.vars=c("run.type")), aes(x=run.type,y=value, color=variable)) + geom_point() + ggtitle("Coeff 2")
ggplot(melt(comp3.pc.B,id.vars=c("run.type")), aes(x=run.type,y=value, color=variable)) + geom_point() + facet_grid(variable ~ .)
ggplot(melt(comp4.pc.B,id.vars=c("run.type")), aes(x=run.type,y=value, color=variable)) + geom_point() + ggtitle("Coeff 4")

```

## NON PCA Multivator Object

In case one needs to explore all the summary statistics, we give here the same approach as with the PCA but on the actual summary statistics. To check for convergency we are just giving results for the first 4 summary statistics, we can add/remove as needed.

```{r nonpcaobject}
### Creating the multivator objects for a non-PCA-based analysis
RS.mdm <- mdm(x.design.long, types = rep(z.variables, each = dim(simpact.z)[1]))
RS.expt <- experiment(mm = RS.mdm, obs = z_obs)

optima.starttime <- proc.time()
RS.opt.a <- optimal_params(RS.expt, option="a")
optima.endtime <- proc.time() - optima.starttime

comp1.B <- data.frame(t(diag(B(RS.opt.a)[,,1])),"a") 
comp2.B <- data.frame(t(diag(B(RS.opt.a)[,,2])),"a") 
comp3.B <- data.frame(t(diag(B(RS.opt.a)[,,3])),"a")
comp4.B <- data.frame(t(diag(B(RS.opt.a)[,,4])),"a") 

names(comp1.B)[names(comp1.B)=="X.a."] <- "run.type" 
names(comp2.B)[names(comp2.B)=="X.a."] <- "run.type" 
names(comp3.B)[names(comp3.B)=="X.a."] <- "run.type" 
names(comp4.B)[names(comp4.B)=="X.a."] <- "run.type" 

RS.opt.var <-  RS.opt.a

optim.check <- proc.time()

# Use the loop to get iterate through different values. So the optimasation process is faster. 

for (iter in seq(100,700, 100)){  
  print (paste("Working on iteration number: ", iter, sep=" ")) 
  RS.opt.b.var.iter <- optimal_params(RS.expt, option="b", start_hp = RS.opt.var, control = list(maxit=iter)) 
  RS.opt.var <- RS.opt.b.var.iter 

  comp1.B.var <- data.frame(t(diag(B(RS.opt.b.var.iter)[,,1])),paste("b",iter,sep = ""))  
  comp2.B.var <- data.frame(t(diag(B(RS.opt.b.var.iter)[,,2])),paste("b",iter,sep = ""))  
  comp3.B.var <- data.frame(t(diag(B(RS.opt.b.var.iter)[,,3])),paste("b",iter,sep = ""))  
  comp4.B.var <- data.frame(t(diag(B(RS.opt.b.var.iter)[,,4])),paste("b",iter,sep = ""))  
 
  names(comp1.B.var)[11] <- "run.type"  
  names(comp2.B.var)[11] <- "run.type"  
  names(comp3.B.var)[11] <- "run.type"  
  names(comp4.B.var)[11] <- "run.type"  
 
  comp1.B <- rbind(comp1.B, comp1.B.var)
  comp2.B <- rbind(comp2.B, comp2.B.var)
  comp3.B <- rbind(comp3.B, comp3.B.var)
  comp4.B <- rbind(comp4.B, comp4.B.var)
}
#check how long this took.
RS.opt.b <- RS.opt.var 

optim.check.conv <- proc.time() - optim.check 

 #See the plot of convergency in the B matrix of coefficients. -->
ggplot(melt(comp1.B,id.vars=c("run.type")), aes(x=run.type,y=value, color=variable)) + geom_point() + ggtitle("Coeff 1") 
ggplot(melt(comp2.B,id.vars=c("run.type")), aes(x=run.type,y=value, color=variable)) + geom_point() + ggtitle("Coeff 2")  
ggplot(melt(comp3.B,id.vars=c("run.type")), aes(x=run.type,y=value, color=variable)) + geom_point() + ggtitle("Coeff 3")  
ggplot(melt(comp4.B,id.vars=c("run.type")), aes(x=run.type,y=value, color=variable)) + geom_point() + ggtitle("Coeff 4")  
```
## Using the Emulator to Explore the Parameter Space on the PCA Method 

```{r exploreemilatorpcs, echo=FALSE} 
n <- 10000 
x.pc.new <- latin.hypercube(n, length(x.variables), names=colnames(x.design))
x.pc.new.long <- x.pc.new[rep(1:nrow(x.pc.new),comp.number.to.use),] 
x.pc.new.long <- as.matrix(x.pc.new.long) 

RS.pc.new.mdm <- mdm(rbind(x.pc.new.long), types = rep(names(z.pc.df)[1:comp.number.to.use], each = n)) 

pred.starttime.pc.mult <- proc.time() 
RS.pc.prediction.opt.a <- multem(x = RS.pc.new.mdm, expt = RS.pc.expt, hp = RS.pc.opt.a)
pred.endtime.pc.mult <- proc.time() - pred.starttime.pc.mult 

pred2.starttime.pc.mult <- proc.time() 
RS.pc.prediction.opt.b <- multem(x = RS.pc.new.mdm, expt = RS.pc.expt, hp = RS.pc.opt.b) 
pred2.endtime.pc.mult <- proc.time() - pred2.starttime.pc.mult

par(mfrow=c(2,comp.number.to.use)) # distribution of pc i -->
for (i in 1:comp.number.to.use){
  hist(RS.pc.prediction.opt.a[((i-1)*n+1):(i*n)], main = paste("Dist of PC",i, sep = " "), xlab = "opt.pc.a") 
} 

par(mfrow=c(2,comp.number.to.use)) 
for (i in 1:comp.number.to.use){
  hist(RS.pc.prediction.opt.b[((i-1)*n+1):(i*n)], main = paste("Dist of PC",i, sep = " "), xlab = "opt.pc.b")
}
```
## Using the Emulator to Explore the Parameter Space for the None PCA Part
```{r exploreemulatornonpca, echo=FALSE}
x.new <- latin.hypercube(n, length(x.variables), names=colnames(x.design))
x.new.long <- x.new[rep(1:nrow(x.new),length(z.variables)),]
x.new.long <- as.matrix(x.new.long)

RS.new.mdm <- mdm(rbind(x.new.long), types = rep(z.variables, each = n))

pred.starttime <- proc.time()
RS.prediction.opt.a <- multem(x = RS.new.mdm, expt = RS.expt, hp = RS.opt.a)
red.endtime <- proc.time() - pred.starttime

pred2.starttime <- proc.time()
RS.prediction.opt.b <- multem(x = RS.new.mdm, expt = RS.expt, hp = RS.opt.b)
pred2.endtime <- proc.time() - pred2.starttime

par(mfrow=c(2,length(z.variables))) # distribution of z.variable i with opt.a
for (i in 1:length(z.variables)){
  hist(RS.prediction.opt.a[((i-1)*n+1):(i*n)], main = paste("Dist of z.var",i, sep = " "), xlab = "opt.a")
}

par(mfrow=c(2,length(z.variables))) # distribution of z.variable i with opt.b 
for (i in 1:length(z.variables)){
  hist(RS.prediction.opt.b[((i-1)*n+1):(i*n)], main = paste("Dist of z.var",i, sep = " "), xlab = "opt.b")
}

```
## Explore Emulator Output

One way of efficiently comparing emulation output with target statistics is to reshape RS.pc.prediction.* as a dataframe. We will do this for both the prediction from method optimal_params option a and b methods.

```{r exploreoutput, echo=FALSE}

prediction.pc.a.df <- data.frame(matrix(RS.pc.prediction.opt.a, nrow = n,
                                      dimnames = list(rownames = 1:n, colnames = names(z.pc.df)[1:comp.number.to.use])))
prediction.pc.b.df <- data.frame(matrix(RS.pc.prediction.opt.b, nrow = n,
                                      dimnames = list(rownames = 1:n, colnames = names(z.pc.df)[1:comp.number.to.use])))

##One way of efficiently comparing emulation output with target statistics is to reshape RS.prediction.* as a dataframe 
prediction.a.df <- data.frame(matrix(RS.prediction.opt.a, nrow = n,
                                       dimnames = list(rownames = 1:n, colnames = names(z.df)[1:length(z.variables)])))
prediction.b.df <- data.frame(matrix(RS.prediction.opt.b, nrow = n, -->
                                        dimnames = list(rownames = 1:n, colnames = names(z.df)[1:length(z.variables)])))

#### sum of squared distances between model statistics and target statistics
# Note: we could normalise (and centralise) these statistics to give them more equal weight in the SumSq

predicted.values <- function(prediction.df, pc = FALSE){
  if(pc == TRUE){
     targets <- predict(z.pc, as.data.frame(targets))
     targets <- as.numeric(targets)[1:4]
   }

  sq.a.array <- as.data.frame(t(apply(prediction.df, 1, function(x) (x - t(targets))^2)))
  names(sq.a.array) <- names(prediction.df)
  SumSq <- as.numeric(rowSums(sq.array))
  x.estimate.row <- which.min(SumSq)
  pridicted.output.values <- cbind(x.estimate.row, prediction.df[x.estimate.row, ])
  return(pridicted.output.values)
}

rbind(targets, predicted.values(prediction.a.df)[2], predicted.values(prediction.pc.a.df)[2])
targets
pred.a <- predicted.values(prediction.a.df)
pred.pc.a <- predicted.values(prediction.pc.a.df, pc = TRUE)
pred.b <- predicted.values(prediction.b.df)
pred.pc.b <- predicted.values(prediction.pc.b.df, pc = TRUE)

# #### And most importantly, the best estimate for the model parameters:
x.estimate.a <- as.numeric(x.new[pred.a$x.estimate.row, ])
x.estimate.pc.a <- as.numeric(x.pc.new[pred.pc.a$x.estimate.row, ])
x.estimate.b <- as.numeric(x.new[pred.b$x.estimate.row, ])
x.estimate.pc.b <- as.numeric(x.pc.new[pred.pc.b$x.estimate.row, ])
#As an example: the value of the first PC for the target statistics:
as.numeric(as.numeric(z.pc$loadings[, 1]) %*% ((as.numeric(as.dataframe(targets) - z.pc$center) / z.pc$scale) )




